{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_images torch.Size([20000, 1, 28, 28]) torch.float32\n",
      "validation_clean torch.Size([2000, 1, 28, 28]) torch.float32\n",
      "validation_noise torch.Size([2000, 1, 28, 28]) torch.float32\n",
      "test_images torch.Size([2000, 1, 28, 28]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "with np.load('denoising-challenge-01-data.npz') as fh:\n",
    "    training_images_clean = torch.tensor(fh['training_images_clean'])\n",
    "    validation_images_noisy = torch.tensor(fh['validation_images_noisy'])\n",
    "    validation_images_clean = torch.tensor(fh['validation_images_clean'])\n",
    "    test_images_noisy = torch.tensor(fh['test_images_noisy'])\n",
    "\n",
    "# TRAINING DATA: CLEAN\n",
    "# 1. INDEX: IMAGE SERIAL NUMBER (20000)\n",
    "# 2. INDEX: COLOR CHANNEL (1)\n",
    "# 3/4. INDEX: PIXEL VALUE (28 x 28)\n",
    "print(\"training_images\", training_images_clean.shape, training_images_clean.dtype)\n",
    "\n",
    "# VALIDATION DATA: CLEAN + NOISY\n",
    "print(\"validation_clean\", validation_images_clean.shape, validation_images_clean.dtype)\n",
    "print(\"validation_noise\",validation_images_noisy.shape, validation_images_noisy.dtype)\n",
    "\n",
    "# TEST DATA: NOISY\n",
    "print(\"test_images\", test_images_noisy.shape, test_images_noisy.dtype)\n",
    "\n",
    "# TRAIN MODEL ON training_images_clean\n",
    "\n",
    "# CHECK YOUR MODEL USING (validation_images_clean, validation_images_noisy)\n",
    "\n",
    "# DENOISE IMAGES (test_images_clean) USING test_images_noisy\n",
    "\n",
    "# # MAKE SURE THAT YOU HAVE THE RIGHT FORMAT\n",
    "# assert test_images_clean.ndim == 4\n",
    "# assert test_images_clean.shape[0] == 2000\n",
    "# assert test_images_clean.shape[1] == 1\n",
    "# assert test_images_clean.shape[2] == 28\n",
    "# assert test_images_clean.shape[3] == 28\n",
    "# \n",
    "# # AND SAVE EXACTLY AS SHOWN BELOW\n",
    "# np.save('test_images_clean.npy', test_images_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb78cff56a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb78cf90f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check one image\n",
    "plt.imshow(training_images_clean[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directories\n",
    "\n",
    "# if not os.path.exists('C:\\\\tmp\\\\autoencoder'):\n",
    "#     os.mkdir('C:\\\\tmp\\\\autoencoder')\n",
    "# if not os.path.exists('C:\\\\tmp\\\\autoencoder\\\\mlp_img'):\n",
    "#     os.mkdir('C:\\\\tmp\\\\autoencoder\\\\mlp_img')\n",
    "\n",
    "\n",
    "# helper function\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    # model\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(64, 12), \n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(12, 3))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(128, 28 * 28), \n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# creating the pytorch instances\n",
    "model = autoencoder()\n",
    "# load trained model\n",
    "# model.load_state_dict(torch.load('C:\\\\tmp\\\\autoencoder\\\\autoencoder.pth'))\n",
    "\n",
    "# define optimizer and loss function\n",
    "learning_rate = 1e-3\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/schuch/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/5], loss:1485.3423\n",
      "epoch [2/5], loss:1395.7169\n",
      "epoch [3/5], loss:1294.8899\n",
      "epoch [4/5], loss:1164.9102\n",
      "epoch [5/5], loss:1069.3375\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "# variables\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# prep data for pytorch format\n",
    "dataset = training_images_clean\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# workflow\n",
    "for epoch in range(num_epochs):\n",
    "    loss_total = 0\n",
    "    for data in dataloader:\n",
    "        \n",
    "        img = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        \n",
    "        \n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        \n",
    "        \n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    # ===================log========================\n",
    "    loss_total += loss.data[0]\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss_total))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, 'c:\\\\tmp\\\\autoencoder\\\\image_{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving trained model\n",
    "torch.save(model.state_dict(), 'C:\\\\tmp\\\\autoencoder\\\\autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss:31.4961\n",
      "proper validation loss:5.5992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/schuch/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION\n",
    "\n",
    "# prep data\n",
    "dataset_val_noise = validation_images_noisy\n",
    "dataset_val_clean = validation_images_clean\n",
    "\n",
    "# workflow\n",
    "with torch.no_grad():\n",
    "    loss_val = 0\n",
    "    for i in range(dataset_val_noise.shape[0]):\n",
    "\n",
    "        img = dataset_val_noise[i]\n",
    "        img = img.view(img.size(0), -1)\n",
    "\n",
    "        img_clean = dataset_val_clean[i]\n",
    "        img_clean = img_clean.view(img_clean.size(0), -1)\n",
    "\n",
    "\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img_clean)\n",
    "\n",
    "\n",
    "        # # ===================backward====================\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        \n",
    "        # ====================loss=======================\n",
    "        loss_val += np.sqrt(np.sum((img.numpy()-img_clean.numpy())**2))\n",
    "\n",
    "\n",
    "    # ===================log========================\n",
    "    loss_val /= 2000\n",
    "    print('validation loss:{:.4f}'.format(loss.data[0]))\n",
    "    print('proper validation loss:{:.4f}'.format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
